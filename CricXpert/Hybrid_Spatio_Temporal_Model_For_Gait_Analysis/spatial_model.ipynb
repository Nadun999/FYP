{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0, ResNet50, DenseNet121, InceptionV3\n",
    "from tensorflow.keras.layers import Dropout, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor, TFAutoModelForImageClassification\n",
    "from transformers import AdamW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting frames from the video clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, output_folder, player_name, num_frames=20):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Cannot open video file {video_path}\")\n",
    "        return\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    interval = total_frames // num_frames if total_frames > num_frames else 1\n",
    "\n",
    "    video_basename = os.path.splitext(os.path.basename(video_path))[0]  # Get video file name without extension\n",
    "    player_folder = os.path.join(output_folder, player_name)\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(player_folder):\n",
    "            os.makedirs(player_folder)\n",
    "    except OSError:\n",
    "        print(f\"Error: Creating directory {player_folder}\")\n",
    "\n",
    "    frame_ids = [int(interval * i) for i in range(num_frames)]\n",
    "    frame_count = 0\n",
    "    saved_frames = 0\n",
    "\n",
    "    while saved_frames < num_frames:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_ids[saved_frames])\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(f\"Failed to read frame at index {frame_ids[saved_frames]} from {video_path}\")\n",
    "            saved_frames += 1\n",
    "            continue\n",
    "\n",
    "        frame_path = os.path.join(player_folder, f\"{video_basename}_frame_{frame_count}.jpg\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        saved_frames += 1\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the extracted frames and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(root_folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "\n",
    "    # Iterate through each player's folder to load images\n",
    "    for player_name in os.listdir(root_folder):\n",
    "        player_folder = os.path.join(root_folder, player_name)\n",
    "        if os.path.isdir(player_folder):\n",
    "            for img_file in os.listdir(player_folder):\n",
    "                if img_file.lower().endswith(valid_extensions):\n",
    "                    img_path = os.path.join(player_folder, img_file)\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is None:\n",
    "                        print(f\"Warning: Could not read image {img_path} - it may be corrupt or in an unsupported format.\")\n",
    "                        continue\n",
    "\n",
    "                    # img = cv2.resize(img, (64, 64))\n",
    "                    # img = cv2.resize(img, (128, 128))  # Resize for MobileNetV2\n",
    "                    img = cv2.resize(img, (224, 224))  # Resize for ViT\n",
    "                    images.append(img)\n",
    "                    labels.append(player_name)  # Label is the player's name\n",
    "\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Encode labels to integers\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_labels = encoder.fit_transform(labels)\n",
    "    categorical_labels = to_categorical(encoded_labels)  # One-hot encoding\n",
    "\n",
    "    num_images = len(images)\n",
    "    print(f\"Number of loaded and preprocessed frames: {num_images}\")\n",
    "    return images, categorical_labels, encoder, num_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_augmentation():\n",
    "    return ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(num_classes):\n",
    "#     model = Sequential([\n",
    "#         Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "#         MaxPooling2D(2, 2),\n",
    "#         Conv2D(64, (3, 3), activation='relu'),\n",
    "#         MaxPooling2D(2, 2),\n",
    "#         Flatten(),\n",
    "#         Dense(128, activation='relu'),\n",
    "#         Dense(num_classes, activation='softmax')\n",
    "#     ])\n",
    "    \n",
    "#     model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(num_classes):\n",
    "#     base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "#     x = base_model.output\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "#     x = Dense(128, activation='relu')(x)\n",
    "#     # x = Dropout(0.5)(x)  # Dropout layer added here\n",
    "#     # x = Dense(128, activation='relu')(x)\n",
    "#     # x = Dropout(0.5)(x)  # Another dropout layer for additional regularization\n",
    "#     predictions = Dense(num_classes, activation='softmax')(x)\n",
    "#     model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "#     for layer in base_model.layers:\n",
    "#         layer.trainable = False  # Freeze the layers of the base model\n",
    "    \n",
    "#     model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(num_classes):\n",
    "#     base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
    "#     x = base_model.output\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "#     x = Dense(128, activation='relu')(x)\n",
    "#     x = Dropout(0.3)(x)  \n",
    "#     predictions = Dense(num_classes, activation='softmax')(x)\n",
    "#     model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#     # Optionally, fine-tune from this layer onwards\n",
    "#     fine_tune_at = 100\n",
    "#     for layer in base_model.layers[:fine_tune_at]:\n",
    "#         layer.trainable = False\n",
    "#     for layer in base_model.layers[fine_tune_at:]:\n",
    "#         layer.trainable = True\n",
    "\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(num_classes):\n",
    "#     base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
    "#     x = base_model.output\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "#     x = Dense(256, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     predictions = Dense(num_classes, activation='softmax')(x)\n",
    "#     model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#     # Freezing the base layers\n",
    "#     for layer in base_model.layers:\n",
    "#         layer.trainable = False\n",
    "\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(num_classes):\n",
    "#     base_model = DenseNet121(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
    "#     x = base_model.output\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "#     x = Dense(256, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     predictions = Dense(num_classes, activation='softmax')(x)\n",
    "#     model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#     # Freezing the base layers\n",
    "#     for layer in base_model.layers:\n",
    "#         layer.trainable = False\n",
    "\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(num_classes):\n",
    "#     base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
    "#     x = base_model.output\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "#     x = Dense(256, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     predictions = Dense(num_classes, activation='softmax')(x)\n",
    "#     model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#     # Freezing the base layers\n",
    "#     for layer in base_model.layers:\n",
    "#         layer.trainable = False\n",
    "\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes):\n",
    "    # Load feature extractor\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "    \n",
    "    # Load the base ViT model\n",
    "    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',\n",
    "                                                      num_labels=num_classes,\n",
    "                                                      id2label={str(i): f'label_{i}' for i in range(num_classes)},\n",
    "                                                      label2id={f'label_{i}': i for i in range(num_classes)})\n",
    "    \n",
    "    # Set all layers to be trainable (or you can choose to freeze some layers)\n",
    "    for layer in model.vit.parameters():\n",
    "        layer.requires_grad = True\n",
    "    \n",
    "    # Compile the model with appropriate optimizer and loss function\n",
    "    optimizer = AdamW(model.parameters(), lr=0.0001)  # AdamW optimizer is often used with Transformers\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model, feature_extractor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_test, y_test, model, encoder):\n",
    "    # Predict the probabilities for the test data\n",
    "    y_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_probs, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Compute the average precision score\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "    for i in range(len(encoder.classes_)):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_probs[:, i])\n",
    "        average_precision[i] = average_precision_score(y_test[:, i], y_probs[:, i])\n",
    "    \n",
    "    mAP = np.mean(list(average_precision.values()))\n",
    "    \n",
    "    # Generate classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=encoder.classes_))\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred, average='macro')  # You can change 'macro' to 'micro' or 'weighted' depending on your needs\n",
    "    print(\"F1 Score (macro-average):\", f1)\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Mean Average Precision (mAP):\", mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training vs. Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training vs. Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded and preprocessed frames: 1749\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7343c11b78e046bcbf271943ad4d37c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445b61c637074b7697b85e4bdf72d5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afedc0d77cdb4c579b6d2e2dc1aff561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compile() got an unexpected keyword argument 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [360], line 51\u001b[0m\n\u001b[1;32m     47\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_encoder_classes.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, encoder\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [360], line 36\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# # Data augmentation\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# datagen = create_data_augmentation()\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Build and train model\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure the model is built for the correct number of classes\u001b[39;00m\n\u001b[1;32m     37\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test))\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [359], line 17\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(num_classes)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Compile the model with appropriate optimizer and loss function\u001b[39;00m\n\u001b[1;32m     16\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)  \u001b[38;5;66;03m# AdamW optimizer is often used with Transformers\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategorical_crossentropy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, feature_extractor\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:2545\u001b[0m, in \u001b[0;36mModule.compile\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2537\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2538\u001b[0m \u001b[38;5;124;03m    Compile this Module's forward using :func:`torch.compile`.\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;124;03m    See :func:`torch.compile` for details on the arguments for this function.\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2545\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: compile() got an unexpected keyword argument 'optimizer'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    root_folder = '/Users/nadunsenarathne/Downloads/Documents/IIT/4th Year/FYP/CricXpert/Datasets/India'\n",
    "    output_frame_folder = '/Users/nadunsenarathne/Downloads/Documents/IIT/4th Year/FYP/CricXpert/Hybrid_Spatio_Temporal_Model_For_Gait_Analysis/extracted_frames'\n",
    "    video_extensions = ('.mp4', '.avi', '.mov', '.mpeg', '.mpg', '.mkv')\n",
    "\n",
    "    # for player_name in os.listdir(root_folder):\n",
    "    #     player_path = os.path.join(root_folder, player_name, 'gait_data')\n",
    "    #     if os.path.isdir(player_path):\n",
    "    #         print(f\"Processing videos for player: {player_name}\")\n",
    "    #         for video_file in os.listdir(player_path):\n",
    "    #             if video_file.endswith(video_extensions):\n",
    "    #                 video_path = os.path.join(player_path, video_file)\n",
    "    #                 print(f\"Extracting frames from: {video_path}\")\n",
    "    #                 extract_frames(video_path, output_frame_folder, player_name, 20)\n",
    "    #             else:\n",
    "    #                 print(f\"Skipped non-video file: {video_file}\")\n",
    "\n",
    "\n",
    "    # Load and preprocess data\n",
    "    X, y, encoder, num_images = load_and_preprocess_data(output_frame_folder)\n",
    "\n",
    "    # Split data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # # Data augmentation\n",
    "    # datagen = create_data_augmentation()\n",
    "\n",
    "    # # Build and train model\n",
    "    # model = build_model(len(encoder.classes_))    \n",
    "    # history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "    #                     steps_per_epoch=len(X_train) // 32,\n",
    "    #                     epochs=10,\n",
    "    #                     validation_data=(X_test, y_test))\n",
    " \n",
    "    # Build and train model\n",
    "    model = build_model(len(encoder.classes_))  # Ensure the model is built for the correct number of classes\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Plot training history\n",
    "    plot_history(history)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate_model(X_test, y_test, model, encoder)\n",
    "\n",
    "    # Save the model and label encoder\n",
    "    model.save('spatial_model.h5')\n",
    "    np.save('label_encoder_classes.npy', encoder.classes_)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 640ms/step\n",
      "The player in the video is: Hardik_Pandya\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Function to extract frames\n",
    "def extract_frames_for_prediction(video_path, output_folder, num_frames=20):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    interval = total_frames // num_frames if total_frames > num_frames else 1\n",
    "    frame_ids = [int(interval * i) for i in range(num_frames)]\n",
    "    frames = []\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "    except OSError:\n",
    "        print(f\"Error: Creating directory {output_folder}\")\n",
    "\n",
    "    frame_count = 0\n",
    "    while frame_count < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count in frame_ids:\n",
    "            resized_frame = cv2.resize(frame, (128, 128))  # Resize frame as per training\n",
    "            frames.append(resized_frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = load_model('spatial_model.h5')\n",
    "\n",
    "# Properly load the LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.load('label_encoder_classes.npy', allow_pickle=True)\n",
    "\n",
    "# Prediction function\n",
    "def predict_player(video_path):\n",
    "    frame_folder = 'temp_frames'\n",
    "    frames = extract_frames_for_prediction(video_path, frame_folder)\n",
    "    if len(frames) == 0:\n",
    "        return \"No frames to analyze.\"\n",
    "\n",
    "    predictions = model.predict(frames)\n",
    "    predicted_class = np.argmax(np.mean(predictions, axis=0))\n",
    "    predicted_player = encoder.inverse_transform([predicted_class])[0]  # Translate label index back to player name\n",
    "\n",
    "    return predicted_player\n",
    "\n",
    "# Example usage\n",
    "video_path = '/Users/nadunsenarathne/Downloads/Documents/IIT/4th Year/FYP/CricXpert/Datasets/untitled folder/3.mov'  # Path to your test video clip\n",
    "result = predict_player(video_path)\n",
    "print(f\"The player in the video is: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
