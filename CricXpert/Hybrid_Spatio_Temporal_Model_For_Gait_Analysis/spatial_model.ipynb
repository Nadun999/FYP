{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pubudusenarathne/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0, ResNet50, DenseNet121, InceptionV3\n",
    "from tensorflow.keras.layers import Dropout, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, accuracy_score\n",
    "               \n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor, TFAutoModelForImageClassification\n",
    "from transformers import AdamW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting frames from the video clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, output_folder, player_name, num_frames=20):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Cannot open video file {video_path}\")\n",
    "        return\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    interval = total_frames // num_frames if total_frames > num_frames else 1\n",
    "\n",
    "    video_basename = os.path.splitext(os.path.basename(video_path))[0]  # Get video file name without extension\n",
    "    player_folder = os.path.join(output_folder, player_name)\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(player_folder):\n",
    "            os.makedirs(player_folder)\n",
    "    except OSError:\n",
    "        print(f\"Error: Creating directory {player_folder}\")\n",
    "\n",
    "    frame_ids = [int(interval * i) for i in range(num_frames)]\n",
    "    frame_count = 0\n",
    "    saved_frames = 0\n",
    "\n",
    "    while saved_frames < num_frames:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_ids[saved_frames])\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(f\"Failed to read frame at index {frame_ids[saved_frames]} from {video_path}\")\n",
    "            saved_frames += 1\n",
    "            continue\n",
    "\n",
    "        frame_path = os.path.join(player_folder, f\"{video_basename}_frame_{frame_count}.jpg\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        saved_frames += 1\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the extracted frames and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_and_preprocess_data(root_folder):\n",
    "#     images = []\n",
    "#     labels = []\n",
    "#     valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "\n",
    "#     # Iterate through each player's folder to load images\n",
    "#     for player_name in os.listdir(root_folder):\n",
    "#         player_folder = os.path.join(root_folder, player_name)\n",
    "#         if os.path.isdir(player_folder):\n",
    "#             for img_file in os.listdir(player_folder):\n",
    "#                 if img_file.lower().endswith(valid_extensions):\n",
    "#                     img_path = os.path.join(player_folder, img_file)\n",
    "#                     img = cv2.imread(img_path)\n",
    "#                     if img is None:\n",
    "#                         print(f\"Warning: Could not read image {img_path} - it may be corrupt or in an unsupported format.\")\n",
    "#                         continue\n",
    "\n",
    "#                     # img = cv2.resize(img, (64, 64))\n",
    "#                     # img = cv2.resize(img, (128, 128))  # Resize for MobileNetV2\n",
    "#                     img = cv2.resize(img, (224, 224))  # Resize for ViT\n",
    "#                     images.append(img)\n",
    "#                     labels.append(player_name)  # Label is the player's name\n",
    "\n",
    "#     images = np.array(images)\n",
    "#     labels = np.array(labels)\n",
    "\n",
    "#     # Encode labels to integers\n",
    "#     encoder = LabelEncoder()\n",
    "#     encoded_labels = encoder.fit_transform(labels)\n",
    "#     categorical_labels = to_categorical(encoded_labels)  # One-hot encoding\n",
    "\n",
    "#     num_images = len(images)\n",
    "#     print(f\"Number of loaded and preprocessed frames: {num_images}\")\n",
    "#     return images, categorical_labels, encoder, num_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_yolo():\n",
    "#     path_to_cfg = \"/Users/nadunsenarathne/Downloads/Documents/IIT/4th Year/FYP/CricXpert/Hybrid_Spatio_Temporal_Model_For_Gait_Analysis/yolov3.cfg\" \n",
    "#     path_to_weights = \"/Users/nadunsenarathne/Downloads/Documents/IIT/4th Year/FYP/CricXpert/Hybrid_Spatio_Temporal_Model_For_Gait_Analysis/yolov3.weights\"\n",
    "#     net = cv2.dnn.readNet(path_to_weights, path_to_cfg)\n",
    "#     layers_names = net.getLayerNames()\n",
    "    \n",
    "#     # Handling different versions of OpenCV which may return different formats\n",
    "#     try:\n",
    "#         output_layers = [layers_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "#     except Exception:\n",
    "#         output_layers = [layers_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "#     return net, output_layers\n",
    "\n",
    "\n",
    "# def yolo_detect(net, image, output_layers, confidence_threshold=0.3):  # Lowered threshold\n",
    "#     height, width, _ = image.shape\n",
    "#     blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), swapRB=True, crop=False)\n",
    "#     net.setInput(blob)\n",
    "#     outputs = net.forward(output_layers)\n",
    "#     boxes = []\n",
    "#     confidences = []\n",
    "\n",
    "#     for output in outputs:\n",
    "#         for detection in output:\n",
    "#             scores = detection[5:]\n",
    "#             class_id = np.argmax(scores)\n",
    "#             confidence = scores[class_id]\n",
    "#             if confidence > confidence_threshold and class_id == 0:\n",
    "#                 center_x = int(detection[0] * width)\n",
    "#                 center_y = int(detection[1] * height)\n",
    "#                 w = int(detection[2] * width)\n",
    "#                 h = int(detection[3] * height)\n",
    "#                 x = int(center_x - w / 2)\n",
    "#                 y = int(center_y - h / 2)\n",
    "\n",
    "#                 if x >= 0 and y >= 0 and (x + w) <= width and (y + h) <= height:\n",
    "#                     boxes.append([x, y, w, h])\n",
    "#                     confidences.append(float(confidence))\n",
    "\n",
    "#     if boxes:\n",
    "#         largest_box = max(boxes, key=lambda b: b[2]*b[3])\n",
    "#         return [largest_box]\n",
    "#     return []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def load_and_preprocess_data(root_folder, net, output_layers):\n",
    "#     images = []\n",
    "#     labels = []\n",
    "#     valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "\n",
    "#     for player_name in os.listdir(root_folder):\n",
    "#         player_folder = os.path.join(root_folder, player_name)\n",
    "#         if os.path.isdir(player_folder):\n",
    "#             for img_file in os.listdir(player_folder):\n",
    "#                 if img_file.lower().endswith(valid_extensions):\n",
    "#                     img_path = os.path.join(player_folder, img_file)\n",
    "#                     img = cv2.imread(img_path)\n",
    "#                     if img is None:\n",
    "#                         print(f\"Failed to read image {img_path}.\")\n",
    "#                         continue\n",
    "\n",
    "#                     boxes = yolo_detect(net, img, output_layers)\n",
    "#                     if not boxes:\n",
    "#                         print(f\"No valid detections for image {img_path}.\")\n",
    "#                         continue\n",
    "\n",
    "#                     for box in boxes:\n",
    "#                         x, y, w, h = box\n",
    "#                         if x < 0 or y < 0 or x+w > img.shape[1] or y+h > img.shape[0] or w <= 0 or h <= 0:\n",
    "#                             print(f\"Skipping invalid box {box} in image {img_path}.\")\n",
    "#                             continue\n",
    "\n",
    "#                         cropped_img = img[y:y+h, x:x+w]\n",
    "#                         resized_img = cv2.resize(cropped_img, (128,128))\n",
    "#                         images.append(resized_img)\n",
    "#                         labels.append(player_name)\n",
    "    \n",
    "#     if not images:\n",
    "#         raise ValueError(\"No images processed. Check the dataset and detection steps.\")\n",
    "\n",
    "#     images = np.array(images)\n",
    "#     labels = np.array(labels)\n",
    "#     encoder = LabelEncoder()\n",
    "#     encoded_labels = encoder.fit_transform(labels)\n",
    "#     categorical_labels = to_categorical(encoded_labels)\n",
    "    \n",
    "#     return images, categorical_labels, encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo():\n",
    "    path_to_cfg = \"/Users/pubudusenarathne/Downloads/Hybrid_Spatio_Temporal_Model_For_Gait_Analysis/yolov3.cfg\" \n",
    "    path_to_weights = \"/Users/pubudusenarathne/Downloads/Hybrid_Spatio_Temporal_Model_For_Gait_Analysis/yolov3.weights\"\n",
    "    net = cv2.dnn.readNet(path_to_weights, path_to_cfg)\n",
    "    layers_names = net.getLayerNames()\n",
    "    try:\n",
    "        output_layers = [layers_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    except Exception:\n",
    "        output_layers = [layers_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    return net, output_layers\n",
    "\n",
    "def yolo_detect(net, image, output_layers, confidence_threshold=0.3):\n",
    "    height, width, _ = image.shape\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outputs = net.forward(output_layers)\n",
    "    boxes = []\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > confidence_threshold and class_id == 0:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                if x >= 0 and y >= 0 and (x + w) <= width and (y + h) <= height:\n",
    "                    boxes.append([x, y, w, h])\n",
    "    return boxes\n",
    "\n",
    "def load_and_preprocess_data(root_folder, net, output_layers):\n",
    "    images = []\n",
    "    labels = []\n",
    "    valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "    for player_name in os.listdir(root_folder):\n",
    "        player_folder = os.path.join(root_folder, player_name)\n",
    "        if os.path.isdir(player_folder):\n",
    "            for img_file in os.listdir(player_folder):\n",
    "                if img_file.lower().endswith(valid_extensions):\n",
    "                    img_path = os.path.join(player_folder, img_file)\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is None:\n",
    "                        continue\n",
    "                    boxes = yolo_detect(net, img, output_layers)\n",
    "                    for box in boxes:\n",
    "                        x, y, w, h = box\n",
    "                        cropped_img = img[y:y+h, x:x+w]\n",
    "                        resized_img = cv2.resize(cropped_img, (224, 224))  # Resize for ViT\n",
    "                        images.append(resized_img)\n",
    "                        labels.append(player_name)\n",
    "    return np.array(images), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_augmentation():\n",
    "    return ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(num_classes):\n",
    "#     model = Sequential([\n",
    "#         Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "#         MaxPooling2D(2, 2),\n",
    "#         Conv2D(64, (3, 3), activation='relu'),\n",
    "#         MaxPooling2D(2, 2),\n",
    "#         Flatten(),\n",
    "#         Dense(128, activation='relu'),\n",
    "#         Dense(num_classes, activation='softmax')\n",
    "#     ])\n",
    "    \n",
    "#     model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(num_classes):\n",
    "#     base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "#     x = base_model.output\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "#     x = Dense(128, activation='relu')(x)\n",
    "#     # x = Dropout(0.5)(x)  # Dropout layer added here\n",
    "#     # x = Dense(128, activation='relu')(x)\n",
    "#     # x = Dropout(0.5)(x)  # Another dropout layer for additional regularization\n",
    "#     predictions = Dense(num_classes, activation='softmax')(x)\n",
    "#     model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "#     for layer in base_model.layers:\n",
    "#         layer.trainable = False  # Freeze the layers of the base model\n",
    "    \n",
    "#     model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(num_classes):\n",
    "#     base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
    "#     x = base_model.output\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "#     x = Dense(128, activation='relu')(x)\n",
    "#     x = Dropout(0.3)(x)  \n",
    "#     predictions = Dense(num_classes, activation='softmax')(x)\n",
    "#     model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#     # Optionally, fine-tune from this layer onwards\n",
    "#     fine_tune_at = 100\n",
    "#     for layer in base_model.layers[:fine_tune_at]:\n",
    "#         layer.trainable = False\n",
    "#     for layer in base_model.layers[fine_tune_at:]:\n",
    "#         layer.trainable = True\n",
    "\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(num_classes):\n",
    "#     base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
    "#     x = base_model.output\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "#     x = Dense(256, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     predictions = Dense(num_classes, activation='softmax')(x)\n",
    "#     model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#     # Freezing the base layers\n",
    "#     for layer in base_model.layers:\n",
    "#         layer.trainable = False\n",
    "\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(num_classes):\n",
    "#     base_model = DenseNet121(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
    "#     x = base_model.output\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "#     x = Dense(256, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     predictions = Dense(num_classes, activation='softmax')(x)\n",
    "#     model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#     # Freezing the base layers\n",
    "#     for layer in base_model.layers:\n",
    "#         layer.trainable = False\n",
    "\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(num_classes):\n",
    "#     base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
    "#     x = base_model.output\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "#     x = Dense(256, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     predictions = Dense(num_classes, activation='softmax')(x)\n",
    "#     model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#     # Freezing the base layers\n",
    "#     for layer in base_model.layers:\n",
    "#         layer.trainable = False\n",
    "\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(num_classes):\n",
    "#     # Load feature extractor\n",
    "#     feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "    \n",
    "#     # Load the base ViT model\n",
    "#     model = TFAutoModelForImageClassification.from_pretrained(\n",
    "#         'google/vit-base-patch16-224-in21k',\n",
    "#         num_labels=num_classes,\n",
    "#         id2label={str(i): f'label_{i}' for i in range(num_classes)},\n",
    "#         label2id={f'label_{i}': i for i in range(num_classes)}\n",
    "#     )\n",
    "    \n",
    "#     # Set all layers to be trainable (or you can choose to freeze some layers)\n",
    "#     for param in model.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "#     # Initialize the optimizer\n",
    "#     optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "#     return model, feature_extractor, optimizer\n",
    "\n",
    "\n",
    "def build_model(num_classes):\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "    # model = TFAutoModelForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=num_classes)\n",
    "    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=num_classes)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model, feature_extractor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(X_test, y_test, model, encoder):\n",
    "#     # Predict the probabilities for the test data\n",
    "#     y_probs = model.predict(X_test)\n",
    "#     y_pred = np.argmax(y_probs, axis=1)\n",
    "#     y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "#     # Compute the average precision score\n",
    "#     precision = dict()\n",
    "#     recall = dict()\n",
    "#     average_precision = dict()\n",
    "#     for i in range(len(encoder.classes_)):\n",
    "#         precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_probs[:, i])\n",
    "#         average_precision[i] = average_precision_score(y_test[:, i], y_probs[:, i])\n",
    "    \n",
    "#     mAP = np.mean(list(average_precision.values()))\n",
    "    \n",
    "#     # Generate classification report\n",
    "#     print(\"Classification Report:\")\n",
    "#     print(classification_report(y_true, y_pred, target_names=encoder.classes_))\n",
    "    \n",
    "#     f1 = f1_score(y_true, y_pred, average='macro')  # You can change 'macro' to 'micro' or 'weighted' depending on your needs\n",
    "#     print(\"F1 Score (macro-average):\", f1)\n",
    "#     print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "#     print(\"Mean Average Precision (mAP):\", mAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_test, y_test, model, encoder):\n",
    "    # Predict the probabilities for the test data\n",
    "    y_probs = model.predict(X_test)['logits']  # Ensure to use the correct key if the output is a dictionary\n",
    "    y_pred = np.argmax(y_probs, axis=-1)  # Using axis=-1 to be dimension agnostic\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Verify dimensions and handling for precision-recall calculations\n",
    "    if y_probs.ndim == 1:\n",
    "        y_probs = np.expand_dims(y_probs, axis=0)\n",
    "\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "    for i in range(len(encoder.classes_)):\n",
    "        if y_test[:, i].ndim == 1:\n",
    "            y_test[:, i] = np.expand_dims(y_test[:, i], axis=0)\n",
    "        if y_probs[:, i].ndim == 1:\n",
    "            y_probs[:, i] = np.expand_dims(y_probs[:, i], axis=0)\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_probs[:, i])\n",
    "        average_precision[i] = average_precision_score(y_test[:, i], y_probs[:, i])\n",
    "\n",
    "    mAP = np.mean(list(average_precision.values()))\n",
    "\n",
    "    # Generate classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=encoder.classes_))\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')  # You can change 'macro' to 'micro' or 'weighted' depending on your needs\n",
    "    print(\"F1 Score (macro-average):\", f1)\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Mean Average Precision (mAP):\", mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training vs. Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training vs. Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     root_folder = '/Users/nadunsenarathne/Downloads/Documents/IIT/4th Year/FYP/CricXpert/Datasets/India'\n",
    "#     net, output_layers = load_yolo()\n",
    "#     output_frame_folder = '/Users/nadunsenarathne/Downloads/Documents/IIT/4th Year/FYP/CricXpert/Hybrid_Spatio_Temporal_Model_For_Gait_Analysis/extracted_frames'\n",
    "#     video_extensions = ('.mp4', '.avi', '.mov', '.mpeg', '.mpg', '.mkv')\n",
    "\n",
    "#     # for player_name in os.listdir(root_folder):\n",
    "#     #     player_path = os.path.join(root_folder, player_name, 'gait_data')\n",
    "#     #     if os.path.isdir(player_path):\n",
    "#     #         print(f\"Processing videos for player: {player_name}\")\n",
    "#     #         for video_file in os.listdir(player_path):\n",
    "#     #             if video_file.endswith(video_extensions):\n",
    "#     #                 video_path = os.path.join(player_path, video_file)\n",
    "#     #                 print(f\"Extracting frames from: {video_path}\")\n",
    "#     #                 extract_frames(video_path, output_frame_folder, player_name, 20)\n",
    "#     #             else:\n",
    "#     #                 print(f\"Skipped non-video file: {video_file}\")\n",
    "\n",
    "\n",
    "#     # # Load and preprocess data\n",
    "#     # X, y, encoder = load_and_preprocess_data(output_frame_folder, net, output_layers)\n",
    "\n",
    "\n",
    "#     # Load and preprocess data\n",
    "#     X, y, encoder, num_images = load_and_preprocess_data(output_frame_folder)\n",
    "\n",
    "#     # Split data into training and testing\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     # # Data augmentation\n",
    "#     # datagen = create_data_augmentation()\n",
    "\n",
    "#     # # Build and train model\n",
    "#     # model = build_model(len(encoder.classes_))    \n",
    "#     # history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "#     #                     steps_per_epoch=len(X_train) // 32,\n",
    "#     #                     epochs=15,\n",
    "#     #                     validation_data=(X_test, y_test))\n",
    " \n",
    "#     # Build and train model\n",
    "#     model = build_model(len(encoder.classes_))  \n",
    "#     history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "#     # Plot training history\n",
    "#     plot_history(history)\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     evaluate_model(X_test, y_test, model, encoder)\n",
    "\n",
    "#     # Save the model and label encoder\n",
    "#     model.save('spatial_model.h5')\n",
    "#     np.save('label_encoder_classes.npy', encoder.classes_)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     root_folder = '/Users/nadunsenarathne/Downloads/Documents/IIT/4th Year/FYP/CricXpert/Datasets/India'\n",
    "#     net, output_layers = load_yolo()\n",
    "#     output_frame_folder = '/Users/nadunsenarathne/Downloads/Documents/IIT/4th Year/FYP/CricXpert/Hybrid_Spatio_Temporal_Model_For_Gait_Analysis/extracted_frames'\n",
    "#     video_extensions = ('.mp4', '.avi', '.mov', '.mpeg', '.mpg', '.mkv')\n",
    "\n",
    "#     # for player_name in os.listdir(root_folder):\n",
    "#     #     player_path = os.path.join(root_folder, player_name, 'gait_data')\n",
    "#     #     if os.path.isdir(player_path):\n",
    "#     #         print(f\"Processing videos for player: {player_name}\")\n",
    "#     #         for video_file in os.listdir(player_path):\n",
    "#     #             if video_file.endswith(video_extensions):\n",
    "#     #                 video_path = os.path.join(player_path, video_file)\n",
    "#     #                 print(f\"Extracting frames from: {video_path}\")\n",
    "#     #                 extract_frames(video_path, output_frame_folder, player_name, 20)\n",
    "#     #             else:\n",
    "#     #                 print(f\"Skipped non-video file: {video_file}\")\n",
    "\n",
    "\n",
    "#     # Load and preprocess data\n",
    "#     X, y, encoder = load_and_preprocess_data(output_frame_folder, net, output_layers)\n",
    "\n",
    "#     # Split data into training and testing\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     # # Data augmentation\n",
    "#     # datagen = create_data_augmentation()\n",
    "\n",
    "#     # # Build and train model\n",
    "#     # model = build_model(len(encoder.classes_))    \n",
    "#     # history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "#     #                     steps_per_epoch=len(X_train) // 32,\n",
    "#     #                     epochs=15,\n",
    "#     #                     validation_data=(X_test, y_test))\n",
    " \n",
    "#     # Build and train model\n",
    "#     model = build_model(len(encoder.classes_))  \n",
    "#     history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "#     # Plot training history\n",
    "#     plot_history(history)\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     evaluate_model(X_test, y_test, model, encoder)\n",
    "\n",
    "#     # Save the model and label encoder\n",
    "#     model.save('spatial_model.h5')\n",
    "#     np.save('label_encoder_classes.npy', encoder.classes_)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m\n\u001b[1;32m     46\u001b[0m     feature_extractor\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_extractor\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m video_extensions \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.avi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mov\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mpeg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mkv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# for player_name in os.listdir(root_folder):\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     player_path = os.path.join(root_folder, player_name, 'gait_data')\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#     if os.path.isdir(player_path):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#             else:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#                 print(f\"Skipped non-video file: {video_file}\")\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_frame_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[1;32m     23\u001b[0m encoded_labels \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(labels)\n",
      "Cell \u001b[0;32mIn[2], line 44\u001b[0m, in \u001b[0;36mload_and_preprocess_data\u001b[0;34m(root_folder, net, output_layers)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img_file\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(valid_extensions):\n\u001b[1;32m     43\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(player_folder, img_file)\n\u001b[0;32m---> 44\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # root_folder = '/Users/nadunsenarathne/Downloads/Documents/IIT/4th Year/FYP/CricXpert/Datasets/India'\n",
    "    net, output_layers = load_yolo()\n",
    "    output_frame_folder = '/Users/pubudusenarathne/Downloads/Hybrid_Spatio_Temporal_Model_For_Gait_Analysis/extracted_frames'\n",
    "    video_extensions = ('.mp4', '.avi', '.mov', '.mpeg', '.mpg', '.mkv')\n",
    "\n",
    "    # for player_name in os.listdir(root_folder):\n",
    "    #     player_path = os.path.join(root_folder, player_name, 'gait_data')\n",
    "    #     if os.path.isdir(player_path):\n",
    "    #         print(f\"Processing videos for player: {player_name}\")\n",
    "    #         for video_file in os.listdir(player_path):\n",
    "    #             if video_file.endswith(video_extensions):\n",
    "    #                 video_path = os.path.join(player_path, video_file)\n",
    "    #                 print(f\"Extracting frames from: {video_path}\")\n",
    "    #                 extract_frames(video_path, output_frame_folder, player_name, 20)\n",
    "    #             else:\n",
    "    #                 print(f\"Skipped non-video file: {video_file}\")\n",
    "\n",
    "\n",
    "    images, labels = load_and_preprocess_data(output_frame_folder, net, output_layers)\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_labels = encoder.fit_transform(labels)\n",
    "    categorical_labels = to_categorical(encoded_labels, num_classes=len(encoder.classes_))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, categorical_labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model, feature_extractor = build_model(len(encoder.classes_))\n",
    "    \n",
    "    X_train = feature_extractor(X_train, return_tensors='tf')['pixel_values']\n",
    "    X_test = feature_extractor(X_test, return_tensors='tf')['pixel_values']\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Plot training history\n",
    "    plot_history(history)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate_model(X_test, y_test, model, encoder)\n",
    "\n",
    "    # Save the model and label encoder\n",
    "    model.save_pretrained('vit_spatial_model', save_format='tf')\n",
    "    np.save('label_encoder_classes.npy', encoder.classes_)\n",
    "\n",
    "    # Save the feature extractor as well since its configuration might be tailored to the model\n",
    "    feature_extractor.save_pretrained('feature_extractor')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 636ms/step\n",
      "The player in the video is: Jasprit_Bumrah\n"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Function to extract frames\n",
    "# def extract_frames_for_prediction(video_path, output_folder, num_frames=20):\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     interval = total_frames // num_frames if total_frames > num_frames else 1\n",
    "#     frame_ids = [int(interval * i) for i in range(num_frames)]\n",
    "#     frames = []\n",
    "\n",
    "#     try:\n",
    "#         if not os.path.exists(output_folder):\n",
    "#             os.makedirs(output_folder)\n",
    "#     except OSError:\n",
    "#         print(f\"Error: Creating directory {output_folder}\")\n",
    "\n",
    "#     frame_count = 0\n",
    "#     while frame_count < total_frames:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         if frame_count in frame_ids:\n",
    "#             resized_frame = cv2.resize(frame, (128, 128))  # Resize frame as per training\n",
    "#             frames.append(resized_frame)\n",
    "#         frame_count += 1\n",
    "\n",
    "#     cap.release()\n",
    "#     return np.array(frames)\n",
    "\n",
    "\n",
    "# # Load the model\n",
    "# model = load_model('spatial_model.h5')\n",
    "\n",
    "# # Properly load the LabelEncoder\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.classes_ = np.load('label_encoder_classes.npy', allow_pickle=True)\n",
    "\n",
    "# # Prediction function\n",
    "# def predict_player(video_path):\n",
    "#     frame_folder = 'temp_frames'\n",
    "#     frames = extract_frames_for_prediction(video_path, frame_folder)\n",
    "#     if len(frames) == 0:\n",
    "#         return \"No frames to analyze.\"\n",
    "\n",
    "#     predictions = model.predict(frames)\n",
    "#     predicted_class = np.argmax(np.mean(predictions, axis=0))\n",
    "#     predicted_player = encoder.inverse_transform([predicted_class])[0]  # Translate label index back to player name\n",
    "\n",
    "#     return predicted_player\n",
    "\n",
    "# # Example usage\n",
    "# video_path = '/Users/nadunsenarathne/Downloads/Documents/IIT/4th Year/FYP/CricXpert/Datasets/untitled folder/5.mov'  # Path to your test video clip\n",
    "# result = predict_player(video_path)\n",
    "# print(f\"The player in the video is: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 634ms/step\n",
      "The player in the video is: Arshdeep_Singh\n"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# def extract_frames_for_prediction(video_path, output_folder, net, output_layers, num_frames=10):\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     interval = total_frames // num_frames if total_frames > num_frames else 1\n",
    "#     frame_ids = [int(interval * i) for i in range(num_frames)]\n",
    "#     frames = []\n",
    "\n",
    "#     try:\n",
    "#         if not os.path.exists(output_folder):\n",
    "#             os.makedirs(output_folder)\n",
    "#     except OSError:\n",
    "#         print(f\"Error: Creating directory {output_folder}\")\n",
    "\n",
    "#     frame_count = 0\n",
    "#     while frame_count < total_frames:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         if frame_count in frame_ids:\n",
    "#             # Apply YOLO detection to each frame\n",
    "#             boxes = yolo_detect(net, frame, output_layers)\n",
    "#             if boxes:  # Check if there is at least one detection\n",
    "#                 x, y, w, h = boxes[0]  # Use the largest detected box (assuming `yolo_detect` returns such)\n",
    "#                 cropped_frame = frame[y:y+h, x:x+w]\n",
    "#                 resized_frame = cv2.resize(cropped_frame, (128, 128))  # Resize frame as per training\n",
    "#                 frames.append(resized_frame)\n",
    "#             else:\n",
    "#                 print(f\"No valid detections at frame {frame_count}.\")\n",
    "#         frame_count += 1\n",
    "\n",
    "#     cap.release()\n",
    "#     return np.array(frames)\n",
    "\n",
    "# # Load the model and YOLO net\n",
    "# model = load_model('spatial_model.h5')\n",
    "# net, output_layers = load_yolo()\n",
    "\n",
    "# # Properly load the LabelEncoder\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.classes_ = np.load('label_encoder_classes.npy', allow_pickle=True)\n",
    "\n",
    "# # Prediction function\n",
    "# def predict_player(video_path):\n",
    "#     frame_folder = 'temp_frames'\n",
    "#     frames = extract_frames_for_prediction(video_path, frame_folder, net, output_layers)\n",
    "#     if frames.size == 0:\n",
    "#         return \"No frames to analyze or no valid detections.\"\n",
    "\n",
    "#     predictions = model.predict(frames)\n",
    "#     predicted_class = np.argmax(np.mean(predictions, axis=0))\n",
    "#     predicted_player = encoder.inverse_transform([predicted_class])[0]  # Translate label index back to player name\n",
    "\n",
    "#     return predicted_player\n",
    "\n",
    "# # Example usage\n",
    "# video_path = '/Users/nadunsenarathne/Downloads/Documents/IIT/4th Year/FYP/CricXpert/Datasets/untitled folder/2.mov'  # Path to your test video clip\n",
    "# result = predict_player(video_path)\n",
    "# print(f\"The player in the video is: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFViTForImageClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "The player in the video is: Axar_Patel\n"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from transformers import ViTFeatureExtractor, TFAutoModelForImageClassification\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# def load_yolo():\n",
    "#     path_to_cfg = \"/Users/nadunsenarathne/Downloads/Documents/IIT/4th Year/FYP/CricXpert/Hybrid_Spatio_Temporal_Model_For_Gait_Analysis/yolov3.cfg\" \n",
    "#     path_to_weights = \"/Users/nadunsenarathne/Downloads/Documents/IIT/4th Year/FYP/CricXpert/Hybrid_Spatio_Temporal_Model_For_Gait_Analysis/yolov3.weights\"\n",
    "#     net = cv2.dnn.readNet(path_to_weights, path_to_cfg)\n",
    "#     layers_names = net.getLayerNames()\n",
    "#     try:\n",
    "#         output_layers = [layers_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "#     except Exception:\n",
    "#         output_layers = [layers_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "#     return net, output_layers\n",
    "\n",
    "# def yolo_detect(net, image, output_layers, confidence_threshold=0.3):\n",
    "#     height, width, _ = image.shape\n",
    "#     blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), swapRB=True, crop=False)\n",
    "#     net.setInput(blob)\n",
    "#     outputs = net.forward(output_layers)\n",
    "#     boxes = []\n",
    "#     for output in outputs:\n",
    "#         for detection in output:\n",
    "#             scores = detection[5:]\n",
    "#             class_id = np.argmax(scores)\n",
    "#             confidence = scores[class_id]\n",
    "#             if confidence > confidence_threshold and class_id == 0:\n",
    "#                 center_x = int(detection[0] * width)\n",
    "#                 center_y = int(detection[1] * height)\n",
    "#                 w = int(detection[2] * width)\n",
    "#                 h = int(detection[3] * height)\n",
    "#                 x = int(center_x - w / 2)\n",
    "#                 y = int(center_y - h / 2)\n",
    "#                 if x >= 0 and y >= 0 and (x + w) <= width and (y + h) <= height:\n",
    "#                     boxes.append([x, y, w, h])\n",
    "#     return boxes\n",
    "\n",
    "# def extract_frames_for_prediction(video_path, output_folder, net, output_layers, num_frames=10):\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     interval = total_frames // num_frames if total_frames > num_frames else 1\n",
    "#     frame_ids = [int(interval * i) for i in range(num_frames)]\n",
    "#     frames = []\n",
    "#     feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "#     try:\n",
    "#         if not os.path.exists(output_folder):\n",
    "#             os.makedirs(output_folder)\n",
    "#     except OSError:\n",
    "#         print(f\"Error: Creating directory {output_folder}\")\n",
    "\n",
    "#     frame_count = 0\n",
    "#     while frame_count < total_frames:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         if frame_count in frame_ids:\n",
    "#             boxes = yolo_detect(net, frame, output_layers)\n",
    "#             if boxes:\n",
    "#                 x, y, w, h = boxes[0]\n",
    "#                 cropped_frame = frame[y:y+h, x:x+w]\n",
    "#                 resized_frame = cv2.resize(cropped_frame, (224, 224))\n",
    "#                 inputs = feature_extractor(images=resized_frame, return_tensors=\"np\")\n",
    "#                 frames.append(inputs['pixel_values'][0])\n",
    "#             else:\n",
    "#                 print(f\"No valid detections at frame {frame_count}.\")\n",
    "#         frame_count += 1\n",
    "\n",
    "#     cap.release()\n",
    "#     return np.array(frames)\n",
    "\n",
    "# def predict_player(video_path):\n",
    "#     frame_folder = 'temp_frames'\n",
    "#     net, output_layers = load_yolo()\n",
    "#     model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "#     encoder = LabelEncoder()\n",
    "#     encoder.classes_ = np.load('label_encoder_classes.npy', allow_pickle=True)\n",
    "#     frames = extract_frames_for_prediction(video_path, frame_folder, net, output_layers)\n",
    "#     if frames.size == 0:\n",
    "#         return \"No frames to analyze or no valid detections.\"\n",
    "\n",
    "#     predictions = model.predict(frames)['logits']\n",
    "#     predicted_class = np.argmax(np.mean(predictions, axis=0))\n",
    "#     predicted_player = encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "#     return predicted_player\n",
    "\n",
    "# video_path = '/Users/nadunsenarathne/Downloads/Documents/IIT/4th Year/FYP/CricXpert/Datasets/untitled folder/4.mov'  # Path to your test video clip\n",
    "# result = predict_player(video_path)\n",
    "# print(f\"The player in the video is: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def extract_frames_for_prediction(video_path, output_folder, net, output_layers, num_frames=10):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    interval = total_frames // num_frames if total_frames > num_frames else 1\n",
    "    frame_ids = [int(interval * i) for i in range(num_frames)]\n",
    "    frames = []\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained('feature_extractor')  # Load locally saved feature extractor\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "    except OSError:\n",
    "        print(f\"Error: Creating directory {output_folder}\")\n",
    "\n",
    "    frame_count = 0\n",
    "    while frame_count < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count in frame_ids:\n",
    "            boxes = yolo_detect(net, frame, output_layers)\n",
    "            if boxes:\n",
    "                x, y, w, h = boxes[0]\n",
    "                cropped_frame = frame[y:y+h, x:x+w]\n",
    "                resized_frame = cv2.resize(cropped_frame, (224, 224))\n",
    "                inputs = feature_extractor(images=resized_frame, return_tensors=\"np\")\n",
    "                frames.append(inputs['pixel_values'][0])\n",
    "            else:\n",
    "                print(f\"No valid detections at frame {frame_count}.\")\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def predict_player(video_path):\n",
    "    frame_folder = 'temp_frames'\n",
    "    net, output_layers = load_yolo()\n",
    "    model = ViTForImageClassification.from_pretrained('vit_spatial_model')  # Load locally saved model\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.classes_ = np.load('label_encoder_classes.npy', allow_pickle=True)\n",
    "    frames = extract_frames_for_prediction(video_path, frame_folder, net, output_layers)\n",
    "    if frames.size == 0:\n",
    "        return \"No frames to analyze or no valid detections.\"\n",
    "\n",
    "    predictions = model.predict(frames)['logits']\n",
    "    predicted_class = np.argmax(np.mean(predictions, axis=0))\n",
    "    predicted_player = encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "    return predicted_player\n",
    "\n",
    "# Example usage\n",
    "video_path = '/path/to/your/video.mov'  # Update this path\n",
    "result = predict_player(video_path)\n",
    "print(f\"The player in the video is: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
